{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d529df1-5863-479f-aa1b-c32c8cb1dae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, TextStreamer\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce222f4",
   "metadata": {},
   "source": [
    "## params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726632b-b052-414e-ab44-b671e67b6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=512\n",
    "dtype = None\n",
    "model_name = \"dsr18b\"  # your model name here\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # use the same tokenizer\n",
    "tokenizer.padding_side = \"right\" # padding side=left(default)\n",
    "# print(\"padding_side:\", tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a1f933",
   "metadata": {},
   "source": [
    "## wandb visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf71045-1bb4-442f-9708-1a72c58394a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"you key here\")\n",
    "run = wandb.init(\n",
    "    project='LoRA SFT in ingratiation dataset',\n",
    "    job_type=\"training\",\n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b6e07",
   "metadata": {},
   "source": [
    "## get response by one request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a38c5-3307-4bd2-9257-878ac97b0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer) # streamer ouput\n",
    "# fix commas in json string\n",
    "def fix_json_commas(json_string):\n",
    "    json_string = re.sub(r'([^,{])(\\\"课程名称\\\")', r'\\1,\\2', json_string)\n",
    "    json_string = re.sub(r'([^,{])(\\\"分类\\\")', r'\\1,\\2', json_string)\n",
    "    json_string = re.sub(r',\\s*}', '}', json_string)\n",
    "    return json_string\n",
    "\n",
    "@torch.no_grad() # shut down the gradient calculation\n",
    "def get_response(input_message):\n",
    "    inputs = tokenizer(input_message, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.3,\n",
    "        repetition_penalty=1.2,\n",
    "        use_cache=True,\n",
    "        do_sample=False,\n",
    "        streamer=streamer,  # streamer \n",
    "    )\n",
    "    print(f\"outputs={outputs}\") # check model outputs\n",
    "\n",
    "    # get the generated response\n",
    "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response_text = response_text.split(\"</think>\")[-1].strip()\n",
    "    \n",
    "    # clean the response\n",
    "    cleaned_res = response_text.replace(input_message, '').strip().replace('```json', '').replace('```', '').replace(' ', '').replace('\\n', '').replace('\\\\\"', '\"')\n",
    "    cleaned_res = re.sub(\"'\", '\"', cleaned_res)\n",
    "    cleaned_res = re.sub(r':\"\"(.*?)\"\"', r':\"\\1\"', cleaned_res)\n",
    "    cleaned_res = fix_json_commas(cleaned_res)\n",
    "\n",
    "    answer = response_text.split(\"</think>\")[-1].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5db409-98a7-4b16-a4a2-ec438e57ac75",
   "metadata": {},
   "source": [
    "## get test for prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95afe2e3-2ce2-47ee-b312-7a7d5b9d614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style = \"\"\"你是教育学专家，正在对学习者在线课堂中的弹幕进行行为分类。\n",
    "[分类规则]={{\n",
    "    \"观点遵从\"：明确或间接地附和教师观点，表达对教师论点或行为的一致认同。\n",
    "    \"恭维他人\"：直接或间接赞美教师、强调教师的重要性与优点。\n",
    "    \"自我展现\"：突出自己的见解、经验或成就，以期引起教师注意。\n",
    "    \"施恩他人\"：表达对教师或同学的帮助意愿，用行动或建议来支持他人。\n",
    "}}\n",
    "### 指令:\n",
    "请只针对本次问题生成输出，课程名称包含场景信息，回答仅输出JSON格式，不在4类分类中请输出\"无\"\n",
    "思考后请严格按照 JSON 输出，禁止输出任何额外文本、注释、思考过程等。只能是 JSON！\n",
    "\n",
    "[输出格式]=\n",
    "{{\n",
    "    \"分类\": str分类\n",
    "}}\n",
    "### 问题:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd0aeb-d0b1-41d1-bc32-1051c7c085c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"{'弹幕内容': '太形象了', '课程名称': '数据结构'}属于什么分类？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ccd80-eba1-45fa-98ab-409ace8f14e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_response(prompt_style.format(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f4f73",
   "metadata": {},
   "source": [
    "## get response in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab344b6-b199-4d4f-a61b-00803bec3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_text(text):\n",
    "    \"\"\"\n",
    "    to get structure of json in {\"分类\": \"XXX\"} \n",
    "    \"\"\"\n",
    "    json_pattern = r'(\\{.*?\"分类\".*?\\})'\n",
    "    matches = re.findall(json_pattern, text, re.DOTALL)\n",
    "    \n",
    "    for match in matches:\n",
    "        try:\n",
    "            parsed_json = json.loads(match)  # from string to json\n",
    "            return parsed_json\n",
    "        except json.JSONDecodeError:\n",
    "            continue  \n",
    "    return None\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_response_batch(questions):\n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.3,\n",
    "        repetition_penalty=1.2,\n",
    "        use_cache=True,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    responses = []\n",
    "    for question, response_text in zip(questions, decoded_outputs):\n",
    "        try:\n",
    "            response_text = response_text.split(\"</think>\")[-1].strip()\n",
    "            cleaned_res = response_text.replace(question, '').strip().replace('```json', '').replace('```', '').replace(' ', '').replace('\\n', '').replace('\\\\\"', '\"')\n",
    "            cleaned_res = re.sub(\"'\", '\"', cleaned_res)\n",
    "            cleaned_res = re.sub(r':\"\"(.*?)\"\"', r':\"\\1\"', cleaned_res)\n",
    "            cleaned_res = fix_json_commas(cleaned_res)\n",
    "            parsed = json.loads(cleaned_res)\n",
    "            if isinstance(parsed, list):\n",
    "                parsed = parsed[0] if parsed else {}\n",
    "            responses.append(parsed)\n",
    "        except json.JSONDecodeError:\n",
    "            # **try get json directly**\n",
    "            extracted_json = extract_json_from_text(cleaned_res)\n",
    "            if extracted_json:\n",
    "                parsed = extracted_json \n",
    "                responses.append(parsed)\n",
    "            else:\n",
    "                eee = {\"error\": \"cannot find effctive json\"}\n",
    "                responses.append({\"error\": str(eee)})\n",
    "        except Exception as e:\n",
    "            responses.append({\"error\": str(e)})\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa831ec8-ae40-4e5c-b9a8-33122ddaec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train, valid, test data\n",
    "with open('Ingratiation/train_data.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('Ingratiation/valid_data.json', 'r', encoding='utf-8') as f:\n",
    "    valid_data = json.load(f)\n",
    "with open('Ingratiation/test_data.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d3a2d-fa09-449a-9309-4c16d7d2cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, data_type=\"train\", BATCH_SIZE=8):\n",
    "    \"\"\"\n",
    "    get responese in batch\n",
    "    :param data: data list with question and response\n",
    "    :param data_type: train, valid, test\n",
    "    :param BATCH_SIZE: batch size\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    error_logs = []\n",
    "    \n",
    "    # Assemble questions in batches and record the results\n",
    "    for i in range(0, len(data), BATCH_SIZE):\n",
    "        # to check the progress\n",
    "        if i % 100 == 0:\n",
    "            print(f\"data {i} is processing...\")\n",
    "        batch_data = data[i : i + BATCH_SIZE]\n",
    "        questions_batch = [prompt_style.format(item[\"Question\"]) for item in batch_data]\n",
    "\n",
    "        # ger responses in batch\n",
    "        batch_responses = get_response_batch(questions_batch)\n",
    "        # print(f\"data {i}：{batch_responses}\")\n",
    "        for j, response_data in enumerate(batch_responses):\n",
    "            original_response = batch_data[j][\"Response\"]\n",
    "            original_response_gt = original_response[\"分类\"]\n",
    "            # error handling\n",
    "            if (\n",
    "                \"error\" in response_data \n",
    "                or not all(key in response_data for key in [\"分类\"])\n",
    "            ):\n",
    "                error_logs.append({\n",
    "                    \"Question\": batch_data[j][\"Question\"],\n",
    "                    \"原分类\": original_response_gt,\n",
    "                    \"错误信息\": response_data.get(\"error\", \"响应缺少必要字段\")\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # results append\n",
    "            results.append({\n",
    "                \"Question\": batch_data[j][\"Question\"],\n",
    "                \"分类\": response_data[\"分类\"],\n",
    "                \"原分类\": original_response_gt\n",
    "            })\n",
    "\n",
    "    # file output\n",
    "    if data_type.lower() == \"train\":\n",
    "        result_file = f\"df_train_{current_time}.xlsx\"\n",
    "        error_file = f\"error_train_log_{current_time}.xlsx\"\n",
    "    elif data_type.lower() == \"after_train\":\n",
    "        result_file = f\"df_after_train_{current_time}.xlsx\"\n",
    "        error_file = f\"error_after_train_log_{current_time}.xlsx\"\n",
    "    elif data_type.lower() == \"after_test\":\n",
    "        result_file = f\"df_after_test_{current_time}.xlsx\"\n",
    "        error_file = f\"error_after_test_log_{current_time}.xlsx\"\n",
    "    else:\n",
    "        result_file = f\"df_test_{current_time}.xlsx\"\n",
    "        error_file = f\"error_test_log_{current_time}.xlsx\"\n",
    "    df_result = pd.DataFrame()\n",
    "    df_error = pd.DataFrame()\n",
    "\n",
    "    # load df\n",
    "    if results:\n",
    "        df_result = pd.DataFrame(results)\n",
    "        df_result.to_excel(result_file, index=False)\n",
    "        print(f\"{data_type} 处理完成，结果已保存至 {result_file}\")\n",
    "    \n",
    "    if error_logs:\n",
    "        df_error = pd.DataFrame(error_logs)\n",
    "        df_error.to_excel(error_file, index=False)\n",
    "        print(f\"{data_type} 处理完成，错误日志已保存至 {error_file}\")\n",
    "\n",
    "    return df_result,df_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea2ac5-b132-4651-8099-5a2fff996176",
   "metadata": {},
   "source": [
    "## LoRA-Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459b11d-be42-4903-bd40-9e8b2d02d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"你是教育学专家，正在对学习者在线课堂中的弹幕进行行为分类。\n",
    "[分类规则]={{\n",
    "    \"观点遵从\"：明确或间接地附和教师观点，表达对教师论点或行为的一致认同。\n",
    "    \"恭维他人\"：直接或间接赞美教师、强调教师的重要性与优点。\n",
    "    \"自我展现\"：突出自己的见解、经验或成就，以期引起教师注意。\n",
    "    \"施恩他人\"：表达对教师或同学的帮助意愿，用行动或建议来支持他人。\n",
    "}}\n",
    "### 指令:\n",
    "请只针对本次问题生成输出，课程名称包含场景信息，回答仅输出JSON格式，不在4类分类中请输出\"无\"\n",
    "思考后请严格按照 JSON 输出，禁止输出任何额外文本、注释、思考过程等，最终答案只能是 JSON！\n",
    "\n",
    "[输出格式]=\n",
    "{{\n",
    "    \"分类\": str分类\n",
    "}}\n",
    "### 问题:\n",
    "{}\n",
    "### 回答:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "### 最终答案：\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c8aeb",
   "metadata": {},
   "source": [
    "mask question part in labels and set cot and output weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8839f3f4-784e-420f-95d2-96b0a90dbe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights \n",
    "w_input = 0.0\n",
    "w_cot = 1.0\n",
    "w_output = 5.0\n",
    "token_length = 512\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    # get think and output part\n",
    "    COT_PATTERN = re.compile(r'<think>\\n.*?\\n</think>', re.DOTALL)\n",
    "    OUTPUT_MARKER = re.compile(r'### 最终答案：\\n')\n",
    "    # get eos token\n",
    "    EOS_TOKEN = tokenizer.eos_token_id if \"tokenizer\" in globals() else \"<EOS>\"\n",
    "\n",
    "    if isinstance(EOS_TOKEN, str):\n",
    "        EOS_TOKEN = tokenizer.convert_tokens_to_ids(EOS_TOKEN)\n",
    "\n",
    "    # get question, cot and response\n",
    "    input_texts = examples[\"Question\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    \n",
    "    processed = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": [],\n",
    "        \"weight_mask\": [],\n",
    "        \"text\": [],\n",
    "        \"cot_positions\": [],  # cot tokens index [start, end]\n",
    "        \"output_positions\": []  # output tokens index [start, end]\n",
    "    }\n",
    "\n",
    "    for i in range(len(input_texts)):\n",
    "        input_text = str(input_texts[i]).strip()\n",
    "        cot = str(cots[i]).strip()\n",
    "        output = str(outputs[i]).strip()\n",
    "        \n",
    "        full_text = train_prompt_style.format(input_text, cot, output)\n",
    "        processed[\"text\"].append(full_text)  \n",
    "\n",
    "        # get cot position\n",
    "        cot_match = COT_PATTERN.search(full_text)\n",
    "        cot_start, cot_end = (cot_match.start(), cot_match.end()) if cot_match else (0, 0)\n",
    "\n",
    "        # get output position\n",
    "        output_match = OUTPUT_MARKER.search(full_text)\n",
    "        out_start = output_match.end() if output_match else len(full_text)\n",
    "        out_end = len(full_text)\n",
    "        \n",
    "        # Tokenization\n",
    "        encoded = tokenizer(\n",
    "            full_text,\n",
    "            return_offsets_mapping=True,  #\n",
    "            truncation=True,\n",
    "            max_length=token_length-1,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        \n",
    "        # transform char position to token position\n",
    "        def char_pos_to_token_pos(char_pos):\n",
    "            for token_idx, (start, end) in enumerate(encoded[\"offset_mapping\"]):\n",
    "                if start <= char_pos < end:  \n",
    "                    return token_idx\n",
    "            return -1  # not found\n",
    "\n",
    "        # get cot and output tokens index\n",
    "        cot_start_token = char_pos_to_token_pos(cot_start)\n",
    "        cot_end_token = char_pos_to_token_pos(cot_end - 1) \n",
    "        out_start_token = char_pos_to_token_pos(out_start)\n",
    "        out_end_token = char_pos_to_token_pos(out_end - 1)\n",
    "\n",
    "        processed[\"cot_positions\"].append([cot_start_token, cot_end_token])\n",
    "        processed[\"output_positions\"].append([out_start_token, out_end_token])\n",
    "        \n",
    "        # initialize labels and weight_mask\n",
    "        weight_mask = [w_input] * len(encoded[\"input_ids\"])\n",
    "        labels = [-100] * len(encoded[\"input_ids\"])\n",
    "\n",
    "        # set cot and output weight\n",
    "        for idx, (start, end) in enumerate(encoded[\"offset_mapping\"]):\n",
    "            if start >= len(full_text):\n",
    "                continue\n",
    "            # <think> and </think> weight setting\n",
    "            is_start_tag = (cot_start <= start < cot_start + 8)  # \"<think>\\n\" in 8 characters\n",
    "            is_end_tag = (cot_end - 9 <= start < cot_end)       # \"\\n</think>\" in 9 characters\n",
    "            # cot weight setting\n",
    "            if start < cot_end and end > cot_start:\n",
    "                if is_start_tag or is_end_tag:\n",
    "                    weight_mask[idx] = w_output  \n",
    "                else:\n",
    "                    weight_mask[idx] = w_cot     \n",
    "                labels[idx] = encoded[\"input_ids\"][idx]\n",
    "\n",
    "            # output weight setting\n",
    "            if start < out_end and end > out_start:\n",
    "                labels[idx] = encoded[\"input_ids\"][idx]\n",
    "                weight_mask[idx] = w_output\n",
    "\n",
    "        # add EOS token\n",
    "        input_ids = encoded[\"input_ids\"]\n",
    "        attention_mask = encoded[\"attention_mask\"]\n",
    "        \n",
    "        if len(input_ids) < max_seq_length:\n",
    "            input_ids.append(EOS_TOKEN)\n",
    "            attention_mask.append(1)\n",
    "            labels.append(EOS_TOKEN)  # calculate EOS的loss\n",
    "            weight_mask.append(w_output)  # get weight of EOS\n",
    "\n",
    "            # extend index\n",
    "            processed[\"output_positions\"][-1][1] += 1 \n",
    "\n",
    "        # Padding\n",
    "        seq_len = len(input_ids)\n",
    "        pad_len = token_length - seq_len\n",
    "        \n",
    "        processed[\"input_ids\"].append(input_ids + [tokenizer.pad_token_id] * pad_len)\n",
    "        processed[\"attention_mask\"].append(attention_mask + [0] * pad_len)\n",
    "        processed[\"labels\"].append(labels + [-100] * pad_len)\n",
    "        processed[\"weight_mask\"].append(weight_mask + [0.0] * pad_len)\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8fb080",
   "metadata": {},
   "source": [
    "## load train and valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0154b7f-49c8-405f-a448-608491123b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=\"Ingratiation/train_data.json\", split=\"train\")\n",
    "valid_dataset = load_dataset(\"json\", data_files=\"Ingratiation/valid_data.json\", split=\"train\")\n",
    "\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "eval_dataset = valid_dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5530fd",
   "metadata": {},
   "source": [
    "## rewrite loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb7c4e-d8e8-4d2c-bd91-0b4bf462ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for cot and output with weighted average loss\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "class CustomSFTTrainer(SFTTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")  # [batch, seq_len]\n",
    "        weight_mask = inputs.get(\"weight_mask\")  # [batch, seq_len]\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            labels=None, # not use model default loss\n",
    "        )\n",
    "        logits = outputs.logits  # [batch, seq_len, vocab_size]\n",
    "        \n",
    "        # shift logits, labels 和 weight_mask [1,2,3,4] -> [1,2,3] & [2,3,4]\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        shift_weight = weight_mask[..., 1:].contiguous()\n",
    "        \n",
    "        # calulate loss\n",
    "        loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "        loss_per_token = loss_fct(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1)\n",
    "        ).view(shift_labels.size())\n",
    "\n",
    "        # get cot and output mask\n",
    "        cot_mask = (torch.abs(shift_weight - 1.0) < 1e-6) & (shift_labels != -100)\n",
    "        out_mask = (torch.abs(shift_weight - 5.0) < 1e-6) & (shift_labels != -100)\n",
    "\n",
    "        # calculate count\n",
    "        cot_count = max(cot_mask.sum().float(), 1e-6)\n",
    "        out_count = max(out_mask.sum().float(), 1e-6)\n",
    "\n",
    "        # loss in different weights\n",
    "        alpha = 12  # coffiecient of output loss\n",
    "        \n",
    "        if cot_mask.any():\n",
    "            cot_loss = loss_per_token[cot_mask].sum()\n",
    "        else:\n",
    "            cot_loss = torch.tensor(0.0, device=loss_per_token.device)\n",
    "\n",
    "        if out_mask.any():\n",
    "            out_loss = loss_per_token[out_mask].sum() * alpha\n",
    "        else:\n",
    "            out_loss = torch.tensor(0.0, device=loss_per_token.device)\n",
    "\n",
    "        # weighted average loss\n",
    "        total_loss = (cot_loss + out_loss) / (cot_count + alpha * out_count)\n",
    "\n",
    "        del outputs, logits, shift_logits, shift_labels, loss_per_token\n",
    "        return (total_loss, None) if return_outputs else total_loss\n",
    "    \n",
    "    # valid dataset also use the same loss function\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only=True, ignore_keys=None):\n",
    "        with torch.no_grad():  # important\n",
    "            loss = self.compute_loss(model, inputs, return_outputs=False)\n",
    "        return (loss, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6edf6",
   "metadata": {},
   "source": [
    "## load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c1ee4-2866-4116-bd0a-32f4e0c3e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "FastLanguageModel.for_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae38a71-1e04-4dc6-95fa-60a9966e259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up training arguments\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\", \n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78700ed7-9fd5-415e-b281-ed932ca7ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=100,\n",
    "    max_steps=375,\n",
    "    learning_rate=3e-5,\n",
    "    optim=\"adamw_hf\",\n",
    "    weight_decay=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    tf32=True,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=125,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=125,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=125,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    seed=42,\n",
    "    output_dir=\"outputs_ingratiation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9d61f-3005-4bf8-bd24-609e1b60f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_data_collator(features):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor([f[\"input_ids\"] for f in features], dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor([f[\"attention_mask\"] for f in features], dtype=torch.long),\n",
    "        \"labels\": torch.tensor([f[\"labels\"] for f in features], dtype=torch.long),\n",
    "        \"weight_mask\": torch.tensor([f[\"weight_mask\"] for f in features], dtype=torch.float),\n",
    "    }\n",
    "\n",
    "trainer = CustomSFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=custom_data_collator,\n",
    "    remove_unused_columns=False,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20978556-6d75-47ef-92bd-a8f7db9022d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"UNSLOTH_RETURN_LOGITS\"] = \"1\"\n",
    "# unsloth returns full logits tensors when propagating forward, which typically consumes a lot of video memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4745b4-393d-4652-8e3d-0011406fc98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# begin training\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687eaea-5a37-4d51-b5bb-0b04aadd225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training stats\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb7d9e-18e4-4eb0-a1b1-af7bb6501e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "new_model_local = \"DeepSeek-R1-Ingratiation-COT-0317\"\n",
    "model.save_pretrained(new_model_local) # Local saving\n",
    "tokenizer.save_pretrained(new_model_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87accb69-cea3-4339-9d27-6fd91105aa92",
   "metadata": {},
   "source": [
    "## use the lora-finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db9bf61-a795-4ca4-b9db-76bf631b505f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"DeepSeek-R1-Ingratiation-COT-0317\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype = dtype,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3eff7c-b9c8-4947-a19f-593dc1763089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get train data answer\n",
    "current_time1 = datetime.now()\n",
    "df_train_after_SFT_result,df_train_after_SFT_error = process_data(train_data, data_type=\"after_train\", BATCH_SIZE=1)\n",
    "current_time2 = datetime.now()\n",
    "print(f'用时{(current_time2-current_time1).total_seconds()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d2dde-aeda-4efb-8abd-18604659ef43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(df_train_after_SFT_result))\n",
    "print(len(df_train_after_SFT_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb014e-d94c-43e4-a804-a6c1cf4f6dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get valid data answer\n",
    "current_time1 = datetime.now()\n",
    "df_valid_after_SFT_result,df_valid_after_SFT_error = process_data(valid_data, data_type=\"after_train\", BATCH_SIZE=1)\n",
    "current_time2 = datetime.now()\n",
    "print(f'用时{(current_time2-current_time1).total_seconds()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df010a8-12a7-421d-aba7-315b69e7c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_valid_after_SFT_result))\n",
    "print(len(df_valid_after_SFT_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c195f24-4845-40f5-bb17-81f771b43bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get test data answer\n",
    "current_time1 = datetime.now()\n",
    "df_test_after_SFT_result,df_test_after_SFT_error = process_data(test_data, data_type=\"after_test\", BATCH_SIZE=1)\n",
    "current_time2 = datetime.now()\n",
    "print(f'用时{(current_time2-current_time1).total_seconds()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5008ae2-16c8-4cec-833c-9e3cb42eb941",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_test_after_SFT_result))\n",
    "print(len(df_test_after_SFT_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1ffc4-8fe9-4f37-a191-549568820c38",
   "metadata": {},
   "source": [
    "## accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b982dc-751b-4971-ad7d-714dcdf98613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    cohen_kappa_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5ddbd-9063-49ef-b61c-0c1399efae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true_labels, pred_labels):\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(true_labels, pred_labels),\n",
    "        'Precision(macro)': precision_score(true_labels, pred_labels, average='macro'),\n",
    "        'Recall(macro)': recall_score(true_labels, pred_labels, average='macro'),\n",
    "        'F1(macro)': f1_score(true_labels, pred_labels, average='macro'),\n",
    "        'Cohen Kappa': cohen_kappa_score(true_labels, pred_labels)\n",
    "    }\n",
    "    return {k: round(v, 4) for k, v in metrics.items()}\n",
    "\n",
    "file_map = {\n",
    "    'before_SFT_train': 'df_train_202503201253.xlsx',\n",
    "    'before_SFT_test': 'df_test_202503201323.xlsx',\n",
    "    'after_SFT_train': 'df_after_train_202503222116.xlsx',\n",
    "    'after_SFT_test': 'df_after_test_202503222130.xlsx'\n",
    "}\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5737e56-ee40-417f-ab38-39ced5997bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for desc, filename in file_map.items():\n",
    "    df = pd.read_excel(filename, engine='openpyxl')\n",
    "    if '原分类' not in df.columns or '分类' not in df.columns:\n",
    "        raise ValueError(f\"{filename} lost '原分类' 或 '分类'\")\n",
    "    metrics = evaluate_model(df['原分类'], df['分类'])\n",
    "    results[desc] = metrics\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\ndiffernce:\")\n",
    "\n",
    "train_diff = results_df.loc['before_SFT_test'] - results_df.loc['before_SFT_train']\n",
    "train_diff.name = 'train diff'\n",
    "test_diff = results_df.loc['after_SFT_test'] - results_df.loc['after_SFT_train']\n",
    "test_diff.name = 'test diff'\n",
    "\n",
    "comparison = pd.concat([train_diff, test_diff], axis=1).T\n",
    "print(comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
